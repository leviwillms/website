# Catching Up

Date: 2025-11-03

Author: Levi Willms

## State of Research

It is now November and I am not much further along in my state of research that I was back in September. I wish I could say that it was not from a lack of effort but alas, it is completely due to my negligence towards the project. It was quite embarrassing today when my lack of progress inevitably became to the topic of discussion in my meeting with Ken. It is appropriate to say I have just not had the time to spend on the thesis, but I can equally admit that I have not put the time in even when those moments come. I still stand at a point of confusion as to what we are actually trying to achieve. This is for two primary reasons,

1. In order to come up with some method of assessing machine output we must have a notion of the format of that output. At least, I cannot grasp how we can do so without that specific notion.
2. ...

I must turn my focus to how a method of assessment can be created. First of which to understand is what we will be assessing against. How do you assess the machine understanding of a given corpus of text? I must admit that I am lost at where to even begin. So begin I must at the beginning and with so little time remaining to actually experiment, theorize, and draw some sort of conclusion. My first step here is to do a crash course on LLM assessment methodologies. How do we create accuracy readings of LLMs? OCR models? What about VLMs? How have others in the past judged the ability of machine algorithms to summarize, or to _know_? These are some fundamental questions I must find answers to. From there I can begin to see what format of output we will need to assess. Then we can create the ideal output and weight the machine output against that.

On another note Ken has asked me to plan a two week period where I can make the thesis my primary focus. He is in dire need of a break and would like to plan accordingly. In this two week period I would like to set personal goals for research and discovery. At the end of which we can be in a position to have some solid work completed and ready to push forward toward the end of the semester. I think it would be best to begin that two week period now. With the midterm and demo out of the way my schedule is pretty clear for work. I can spend the next couple of weeks just focusing completely on the thesis for school and some development sprints for work.

## Next Steps

As noted above there are a number of avenues in which I must explore. At the bare minimum my goal is to have a better understanding of judging LLM outputs at the end of this week. My hope is that by next week I can choose a dataset to create the real baseline truth to which we can assess against.

## Notes

If I am to use some kind of QA (question/answer) to evaluate the LLM, how is that question/answer data set created? Can the system streamline that?

- Can we automate the creation of baseline truth?
- The problem with creating question/answer methods is that it requires the pre-training of the model and not directly testing the content that was generated. HOW DO WE ASSESS THE CONTENT?

Create a graph from the lecture content.
let a node represent a concept in the textbook (source text for abstraction). Two nodes are connected if they appear in the same chapter together. A weighted edge represents how many times two nodes appear together in separate chapters?

Should we use directed edges?
Does two nodes appearing in the same chapter mean they are related?

How do we measure the assumptions of concepts? For example, multiplication may have the most edges but it is not a topic that a Calculus book will teach.

We will need to define what a concept (or topic) is and how that is determined from a textbook. How do I know Integrals are a concept from Calculus? How do I know that definite integrals are also a topic? Integrals appear in definite integrals, so those two share an edge.

It seems that I can follow the pattern of a compiler. If we do the following:

- Lexical Analysis
  - Break down the source into a stream of tokens, or in our case, a parsed data of content types.
  - Done by ppx
- Parse content data
  - Parse content data into some kind of concept-map AST.
  - Middleware
- Perform analysis
  - Use something like a static analyser to understand and manipulate the source from the AST.
  - Done by Assessment Framework

NOTE: that instead of using AST's (due to the complexity of the textbooks) we can use DAGs- Directed Acyclic Graphs.
