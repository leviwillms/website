# Ditching the Traditional `Golden Set` Methodology

## Update

I have begun with the task of creating a definite _golden_ standard dataset for comparisons. To speed up the process I wanted to make use of OpenAI api to reduce the effort of manual generation. However, OpenAI does still pose some problems in that it has completed some incorrect reviews. The number of false positives (or false negatives) is relatively low. For that matter, we can say that the below results are fairly accurate. Though, I would not say _fairly_ accurate is the desired outcome of the analysis...

```bash
(.venv) levi@ppx:~/ppx$ ppx verify-alignments output/alignments.csv -o output/ground_truth.json
Loading alignments from output/alignments.csv...
Loaded 945 mappings
  Batch 1: 50 results
  Batch 2: 50 results
  Batch 3: 50 results
  Batch 4: 50 results
  Batch 5: 50 results
  Batch 6: 50 results
  Batch 7: 50 results
  Batch 8: 50 results
  Batch 9: 50 results
  Batch 10: 50 results
  Batch 11: 50 results
  Batch 12: 50 results
  Batch 13: 50 results
  Batch 14: 50 results
  Batch 15: 50 results
  Batch 16: 50 results
  Batch 17: 50 results
  Batch 18: 50 results
  Batch 19: 45 results
  Batch 19/19... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100%

Saved 945 verification results to output/ground_truth.json

Summary:
  Correct:   912 (96.5%)
  Incorrect: 33 (3.5%)
```

## Analysis

I've picked a single incorrect mapping with a relatively high score to gain some insight into why it is being incorrectly mapped. After testing the individual token results, the following has been found.

### Mapping

| text_fragment_id | text           | mistral_fragment_id | text                                                                                                                                                                                                                                                                    | score | confidence |
| ---------------- | -------------- | ------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- | ---------- |
| 1989             | "mann, 1992.2" | 600                 | "- [13] Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In David S. Touretzky, editor, Advances in Neural Information Processing Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989], pages 598–605. Morgan Kaufmann, 1989." | 6.0   | 0.75       |

The ppx text fragment maps to the wrong mistral text fragment. The correct text fragment should be below:

| text_fragment_id | text                                                                                                                                                                                                                                                                                                                                                   |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 306              | "- [12] Babak Hassibi and David G. Stork. Second order derivatives for network pruning: Optimal brain surgeon. In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, Advances in Neural Information Processing Systems 5, [NIPS Conference, Denver, Colorado, USA, November 30 - December 3, 1992], pages 164–171. Morgan Kaufmann, 1992." |

### Summary

I ran both strings through the alignment algorithm and found the root cause:

### Test Results

Candidate Score Normalized Span Tokens Matched
| Candidate | Score | Normalized | Span | Tokens Matched |
|-----------|-------|------------|------|----------------|
| WRONG (1989) | 6/8 | 0.75 | 86 | `[CLS]`, `[SEP]`, `##n`, `,`, `.`, `2` |
| CORRECT (1992) | 6/8 | 0.75 | 96 | `1992`, `[CLS]`, `[SEP]`, `,`, `.`, `##n` |

**Why wrong wins:** Both score identically (0.75), but the wrong candidate has a shorter span (86 < 96), so it wins the tie-breaker.

### The Core Problem

The algorithm treats all tokens equally:

| Token                  | Issue                                                                  |
| ---------------------- | ---------------------------------------------------------------------- |
| `[CLS]`, `[SEP]`       | Always present in both, contributes 2 free matching points             |
| Punctuation (`,`, `.`) | High-frequency noise tokens                                            |
| `2`                    | A single digit that appears incidentally in "Systems 2", "27-30", etc. |
| `1992`                 | The semantically critical token gets the same weight as garbage        |

The correct candidate matches 1992 (the key identifier), while the wrong matches 2 from random occurrences. Both get the same score because the algorithm doesn't distinguish token importance.

## Adjustments

To address the issue I've implemented the following:

- IDF for token weights in the alignment scoring.
- Removal of BERT special token IDs ([CLS], [SEP], [MASK], [UNK], [PAD])
- Combining multiple signals
  - Token match score (current algorithm)
  - Key token presence (bonus for matching numbers/proper nouns)
  - Contiguous match bonus (tokens matching in sequence)
- Character-Level Alignment for Short Fragments (Medium effort, High impact)
  - For PPX fragments below a threshold (e.g., 5 words), use character-level fuzzy matching
- Require Key Token Matching (Low effort, Medium impact)
  - For short fragments, require that "distinctive" tokens (numbers, proper nouns) must match
