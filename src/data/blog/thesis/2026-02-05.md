# PPX Hybrid Retrieval: Evaluation Findings

## PPX Hybrid Retrieval: Evaluation Findings

Test document: **MobileNetV2** (`paper_masked`), 10 pages, 281 Mistral fragments, 930 alignments (run 8).

---

### 1. Alignment Fixes

#### Token Normalization

The simple tokenizer treated `"3.1."` and `"3.1"` as different tokens, causing section headings to misalign. Added `normalize_tokens()` that strips trailing punctuation before hashing.

**Before:** PPX fragment `"3.1. Depthwise Separable Convolutions"` aligned to a paragraph (452) instead of the heading (450). Both scored 3/4 — paragraph won by iteration order.

**After:** Heading scores 4/4 (perfect match), paragraph scores 3/4. Heading wins correctly.

#### Length-Based Tie-Breaking

When DP alignment scores are equal, the system now prefers the Mistral fragment closest in token count to the PPX fragment. This resolves remaining ties in favor of structurally similar content.

#### Primary-Only Bbox

Long Mistral paragraphs mapped to 11-18 PPX fragments, creating page-spanning bounding boxes. Changed `_get_bbox_for_fragment()` to use only the highest-confidence aligned PPX fragment, producing tight single-line bboxes.

#### Fallback Penalty

Applied score penalties for imprecise localizations before `min_score` filtering:

| Fallback Type  | Penalty |
| -------------- | ------- |
| `primary_only` | 0.85x   |
| `full_page`    | 0.50x   |

**Result:** Run 8 produced 930 alignments (up from 882 in run 7).

---

### 2. Evaluation Framework

12 ground-truth queries across 6 categories, with 3-tier graded relevance (0=irrelevant, 1=supporting, 2=exact target) and bbox annotations for <span data-tip="Intersection over Union">IoU</span>
measurement.

#### Aggregate Results (bm25_weight=0.3, k=5)

| Metric                                                                                                                       | Value |
| ---------------------------------------------------------------------------------------------------------------------------- | ----- |
| <span data-tip="Precision at 5 - Fraction of top 5 results that are relevant">P@5</span>                                     | 0.43  |
| <span data-tip="Recall at 5 - Fraction of all relevant items found in top 5">R@5</span>                                      | 0.88  |
| <span data-tip="Mean Reciprocal Rank - Average of 1/rank of first relevant result">MRR</span>                                | 0.81  |
| <span data-tip="Normalized Discounted Cumulative Gain at 5 - Measures ranking quality with graded relevance">NDCG@5</span>   | 0.682 |
| <span data-tip="Mean Intersection over Union - Average overlap between predicted and ground-truth bounding boxes">mIoU</span>| 0.58  |

Fallback breakdown (60 total results): 55 direct, 5 primary_only, 0 full_page.

#### Per-Category Breakdown

| Category       | P@5  | MRR  | NDCG  | Queries |
| -------------- | ---- | ---- | ----- | ------- |
| definition     | 0.80 | 1.00 | 1.068 | 1       |
| theory         | 0.60 | 1.00 | 0.964 | 1       |
| architecture   | 0.50 | 0.75 | 0.849 | 2       |
| results        | 0.33 | 0.83 | 0.667 | 3       |
| concept        | 0.47 | 0.58 | 0.559 | 3       |
| implementation | 0.20 | 1.00 | 0.386 | 2       |

Definitions and theory queries perform best. Implementation queries have low precision — the system finds the right page (MRR=1.0) but returns many loosely related fragments.

---

### 3. BM25 Weight Ablation

| Weight  | P@5       | R@5       | MRR       | NDCG      |
| ------- | --------- | --------- | --------- | --------- |
| 0.0     | 0.433     | 0.875     | 0.739     | 0.657     |
| 0.1     | 0.417     | 0.875     | 0.799     | 0.678     |
| 0.3     | 0.433     | 0.875     | 0.812     | 0.682     |
| **0.5** | **0.467** | **0.875** | **0.861** | **0.734** |
| 0.7     | 0.450     | 0.833     | 0.847     | 0.718     |
| 1.0     | 0.450     | 0.833     | 0.799     | 0.697     |

**Optimal: bm25_weight=0.5** (NDCG 0.734 vs 0.657 dense-only, +11.7%).

BM25 primarily improves MRR (+16.5% from 0.739 to 0.861) — lexical matching pushes exact keyword matches to rank 1. Pure BM25 (1.0) degrades recall from 0.875 to 0.833. The hybrid at 0.5 balances both.

---

### 4. Known Limitations

- **NDCG > 1.0** on some queries: the ideal DCG uses expected relevance grades that may not all appear in the result set, inflating the ratio slightly.
- **Single-document benchmark:** ground truth is specific to `paper_masked`. Future work: load ground truth from JSON for arbitrary documents.
- **mIoU coverage:** only 8/12 queries have bbox annotations. Queries without bboxes report `None`.
- **Implementation category weakness:** P@5=0.20 suggests the system retrieves semantically similar but off-topic fragments for procedural queries.
