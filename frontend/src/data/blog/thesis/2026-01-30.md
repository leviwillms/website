# Regression in Alignment

## Changing the algorithm again

I've made further changes to the alignment algorithm as well as the `ppx` system to store alignments as runs. This way I take a snapshot of each algorithm used for each run and can iterate back to which alignment algorithms worked best.

One of the additions to the ppx system was adding the `compare-runs` command in `typr`. This allows me to take two runs that _potentially_ use different algorithms and compare their results. A comparison between runs looks as follows:

```bash
(.venv) levi@ppx:~/ppx$ ppx compare-runs 5 4
Comparing Run 5 (baseline-no-context) vs Run 4 (with-context)

Comparison: Run 5 (baseline-no-context) → Run 4 (with-context)

                 Overview
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┓
┃ Category                        ┃ Count ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━┩
│ Total PPX fragments (A)         │   945 │
│ Total PPX fragments (B)         │   942 │
│ Mapping changed                 │   684 │
│ Fixed (incorrect → correct)     │    28 │
│ Regressed (correct → incorrect) │    34 │
│ Stayed correct                  │   874 │
│ Stayed incorrect                │     4 │
│ Only in A                       │     5 │
│ Only in B                       │     2 │
└─────────────────────────────────┴───────┘

   Alignment Method Breakdown (Run 4 (with-context))
┏━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┓
┃ Method     ┃ Total ┃ Correct ┃ Incorrect ┃ Accuracy ┃
┡━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━┩
│ dp         │   922 │     899 │        23 │    97.5% │
│ dp_context │    20 │       4 │        16 │    20.0% │
└────────────┴───────┴─────────┴───────────┴──────────┘

Fixed Alignments (28):

┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓
┃ PPX ID   ┃ PPX Text                            ┃ Mistral A→B    ┃ Method B     ┃ Conf A→B       ┃
┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩
│ 1566     │ 5.1. Memory efi cient inference     │ 565→512        │ dp           │ 0.54→0.50      │
│ 1598     │ teger L ≥ 1 and p > 1 there exis... │ 548→694        │ dp           │ 1.00→0.83      │
│ 1616     │ that using linear layers is cruc... │ 577→478        │ dp           │ 1.00→1.00      │
│ 1628     │ nel, it inevitably loses informa... │ 645→471        │ dp           │ 1.00→0.73      │
│ 1632     │ in the activation manifold that ... │ 660→471        │ dp           │ 0.67→1.00      │
│ 1639     │ of the activation space then the... │ 548→471        │ dp           │ 0.60→1.00      │
│ 1640     │ preserves the information while ... │ 669→471        │ dp           │ 0.83→1.00      │
│ 1646     │ However, inspired by the intuiti... │ 679→484        │ dp           │ 0.80→1.00      │
│ 1660     │ Table 4 with the full performanc... │ 513→548        │ dp           │ 0.50→1.00      │
│ 1829     │ Optimal brain damage. In David S... │ 696→600        │ dp           │ 0.50→1.00      │
│ 1831     │ editor, Advances in Neural Infor... │ 458→600        │ dp           │ 1.00→1.00      │
│ 1857     │ 2                                   │ 513→689        │ dp           │ 0.67→1.00      │
│ 1865     │ ing deep neural networks with de... │ 527→602        │ dp           │ 0.71→0.96      │
│ 1889     │ faster, stronger.                   │ 480→622        │ dp           │ 1.00→1.00      │
│ 1905     │ [37] Jifeng Dai, Yi Li, Kaiming ... │ 606→624        │ dp           │ 0.80→1.00      │
│ 1927     │ 6, 2012, Lake Tahoe, Nevada, Uni... │ 603→635        │ dp           │ 0.80→1.00      │
│ 1971     │ leaved linear transformation and... │ 541→649        │ dp           │ 0.80→0.84      │
│ 1989     │ a bottleneck residual block as a... │ 600→523        │ dp           │ 0.75→1.00      │
│ 2022     │ learning frameworks. It remains ... │ 615→537        │ dp           │ 1.00→1.00      │
│ 2027     │ Using n = t we end up having to ... │ 541→535        │ dp           │ 0.80→1.00      │
└──────────┴─────────────────────────────────────┴────────────────┴──────────────┴────────────────┘
... and 8 more

Regressed Alignments (34):

┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓
┃ PPX ID   ┃ PPX Text                            ┃ Mistral A→B    ┃ Method B     ┃ Conf A→B       ┃
┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩
│ 1565     │ i.                                  │ 514→647        │ dp           │ 1.00→1.00      │
│ 1574     │ |                                   │ 548→529        │ dp           │ 1.00→1.00      │
│ 1577     │ |Σn| = ∑m−n                         │ 548→608        │ dp_context   │ 1.00→0.89      │
│ 1578     │ m−n (                               │ 548→683        │ dp_context   │ 1.00→0.73      │
│ 1585     │ ≥1−                                 │ 552→565        │ dp           │ 1.00→0.59      │
│ 1586     │ mn+1                                │ 552→592        │ dp_context   │ 1.00→0.94      │
│ 1588     │ 2m                                  │ 552→689        │ dp           │ 1.00→0.72      │
│ 1592     │ bility.                             │ 554→610        │ dp_context   │ 0.80→0.92      │
│ 1805     │ 2015.1                              │ 623→419        │ dp_context   │ 0.80→0.88      │
│ 1876     │ 1                                   │ 639→683        │ dp           │ 0.82→1.00      │
│ 1885     │ 2                                   │ 531→689        │ dp           │ 0.89→1.00      │
│ 1929     │ 9                                   │ 535→565        │ dp           │ 0.83→1.00      │
│ 1959     │ tection, In CVPR, 2015. 7           │ 588→594        │ dp           │ 1.00→0.82      │
│ 1967     │ desired. d                          │ 588→516        │ dp_context   │ 0.90→0.97      │
│ 1982     │ op∈G                                │ 599→552        │ dp           │ 0.94→0.46      │
│ 1985     │ (2)                                 │ 590→565        │ dp           │ 1.00→1.00      │
│ 2002     │ Rs(c× s                             │ 592→527        │ dp           │ 0.88→0.81      │
│ 2016     │ as                                  │ 594→469        │ dp_context   │ 1.00→1.00      │
│ 2132     │ ments.                              │ 421→623        │ dp_context   │ 0.80→0.94      │
│ 2149     │ malization during training.         │ 501→435        │ dp_context   │ 0.62→0.92      │
└──────────┴─────────────────────────────────────┴────────────────┴──────────────┴────────────────┘
... and 14 more
(.venv) levi@ppx:~/ppx$
```

So, after making the adjustments to add alignment test runs in the database and adding a context window to the algorithm we've regressed in correct alignment matching. I will need to investigate this and find the true cause.

## Analysis

Three issues explain the regressions:

### 1. The dp_context fallback has a design flaw (16 of 20 incorrect = 80% error rate)

Looking at the new `align_with_context()` function. It concatenates ALL sibling TextFragments in a `PageElement` into one big `context_text`, then finds the single best-matching Mistral fragment for that entire context. It then assigns that Mistral fragment back to the individual PPX fragment that triggered the fallback.

**The problem**: if a `PageElement` contains 10 fragments, the context match finds a Mistral fragment that best covers the whole group — which is usually a long paragraph. But that paragraph is likely the right match for the longest sibling, not the short ambiguous fragment (like `"ments."` or `"2015.1"`) that triggered the fallback. The short fragment gets a confident-looking score (high because the context matched well) but the wrong Mistral assignment.

This is why `dp_context` shows 20% accuracy vs dp's 97.5%.

### 2. The old CSV used count-based scoring, run 4 uses IDF-weighted scoring — so 684 mappings changed in the base dp path too

Look at the old CSV's scores:

match_score: 5.0, 9.0 ← integer values = raw token counts
With IDF weighting, match_score becomes a sum of log(1 + N/(1+df)) values — non-integer, and the ranking of candidates changes because rare tokens now outweigh common ones. This shifts which Mistral fragment wins for many PPX fragments, even when the fallback isn't involved.

The 684 changed mappings are mostly from IDF re-ranking, not from the context fallback (only 20 used dp_context). Most of those IDF changes are neutral or positive (874 stayed correct, 28 fixed), but 34 regressed — cases where the old count-based tiebreaker happened to pick the right fragment and IDF's re-weighting now picks wrong.

### 3. Several regressions are very short symbolic fragments where any algorithm struggles

Looking at the regressed list: `"i."`, `"|"`, `"≥1−"`, `"mn+1"`, `"2m"`, `"1"`, `"2"`, `"9"`, `"(2)"`, `"op∈G"`, `"as"` — these are 1-2 token fragments that match dozens of Mistral candidates equally well. With count-based scoring they happened to land on the right fragment; with IDF they land differently. These are inherently ambiguous and any change to scoring will shuffle them.

### Net assessment: IDF Weights Prove Useful in Longer Spans

IDF weighting itself is a net positive (+28 fixed, mostly meaningful text fragments) but introduces noise on ultra-short fragments (-34 regressed, mostly symbols)
The dp_context fallback is actively harmful and should be disabled or redesigned — it assigns the context-level best match to the individual fragment, which is the wrong granularity.

## Improvement Techniques

### 1. Disable dp_context fallback (immediate, removes ~16 errors)

The fallback is net-negative. Disabling it takes run 4's dp accuracy of 97.5% and removes the 20% accuracy dp_context results. This alone likely makes run 4 better than the baseline.

### 2. Run a clean IDF-only comparison on paper_masked

Right now you can't tell how much IDF alone helps because the comparison is polluted by dp_context. After disabling the fallback, run on paper_masked, verify, and compare against run 5. That gives you the true IDF delta.

### 3. Replace context fallback with neighbor-constrained re-alignment

The current approach fails because it assigns a context-level match to an individual fragment. A better design:

- After the initial alignment pass completes for all fragments, do a second pass over low-confidence/short fragments
- Look at which Mistral IDs the neighboring PPX fragments (by page order) already mapped to
- Restrict the candidate set to Mistral fragments near those neighbors (e.g., within ±5 in document order)
- Re-run the individual DP alignment against only those narrowed candidates

This uses spatial context for **candidate selection** rather than scoring — the fragment still gets scored on its own tokens, just against a smaller, locality-informed candidate pool.

### 4. Enforce monotonic ordering as a post-processing pass

PPX fragments and their Mistral matches should be roughly monotonic within a page (if PPX A is above PPX B, their Mistral matches should be in order too). After alignment, you could detect ordering violations and re-align the violating fragments against Mistral candidates that restore monotonicity. This would catch many of the short-fragment errors where "1", "2", "(2)" land on random distant Mistral fragments.

## Post-implementation

```bash
(.venv) levi@ppx:~/ppx$ ppx alignment paper_masked 0-13 -v "idf-neighbor-monotonic" -d "IDF + neighbor refinement + monotonic correction"
Pass 1: DP alignment ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Pass 2: Neighbor refinement (56 fragments) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Pass 3: Ordering correction ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00

Alignment run ID: 6
Version: idf-neighbor-monotonic
Alignment complete:
Total PPX fragments processed: 947
Fragments with alignments: 939
Match rate: 99.16%
Total time elapsed: 321.54s

Confidence Distribution:
Min: 0.4623
Max: 1.0000
Mean: 0.9119
Median: 0.9638

Fragments with confidence >= 0.45: 939/947
Pass 2 neighbor refinements: 43
Pass 3 monotonic corrections: 3
```

Now, doing a full run comparison we get:

```bash
(.venv) levi@ppx:~/ppx$ ppx compare-runs 5 6
Comparing Run 5 (baseline-no-context) vs Run 6 (idf-neighbor-monotonic)

Comparison: Run 5 (baseline-no-context) → Run 6 (idf-neighbor-monotonic)

                 Overview
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┓
┃ Category                        ┃ Count ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━┩
│ Total PPX fragments (A)         │   945 │
│ Total PPX fragments (B)         │   939 │
│ Mapping changed                 │   678 │
│ Fixed (incorrect → correct)     │    29 │
│ Regressed (correct → incorrect) │    23 │
│ Stayed correct                  │   883 │
│ Stayed incorrect                │     2 │
│ Only in A                       │     8 │
│ Only in B                       │     2 │
└─────────────────────────────────┴───────┘

            Alignment Method Breakdown (Run 6
                (idf-neighbor-monotonic))
┏━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┓
┃ Method       ┃ Total ┃ Correct ┃ Incorrect ┃ Accuracy ┃
┡━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━┩
│ dp           │   898 │     887 │        11 │    98.8% │
│ dp_monotonic │     3 │       3 │         0 │   100.0% │
│ dp_neighbor  │    38 │      24 │        14 │    63.2% │
└──────────────┴───────┴─────────┴───────────┴──────────┘

Fixed Alignments (29):

┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓
┃ PPX ID   ┃ PPX Text                            ┃ Mistral A→B    ┃ Method B     ┃ Conf A→B       ┃
┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩
│ 1566     │ 5.1. Memory efi cient inference     │ 565→512        │ dp           │ 0.54→0.50      │
│ 1598     │ teger L ≥ 1 and p > 1 there exis... │ 548→694        │ dp           │ 1.00→0.83      │
│ 1616     │ that using linear layers is cruc... │ 577→478        │ dp           │ 1.00→1.00      │
│ 1628     │ nel, it inevitably loses informa... │ 645→471        │ dp           │ 1.00→0.73      │
│ 1632     │ in the activation manifold that ... │ 660→471        │ dp           │ 0.67→1.00      │
│ 1639     │ of the activation space then the... │ 548→471        │ dp           │ 0.60→1.00      │
│ 1640     │ preserves the information while ... │ 669→471        │ dp           │ 0.83→1.00      │
│ 1646     │ However, inspired by the intuiti... │ 679→484        │ dp           │ 0.80→1.00      │
│ 1660     │ Table 4 with the full performanc... │ 513→548        │ dp           │ 0.50→1.00      │
│ 1829     │ Optimal brain damage. In David S... │ 696→600        │ dp           │ 0.50→1.00      │
│ 1831     │ editor, Advances in Neural Infor... │ 458→600        │ dp           │ 1.00→1.00      │
│ 1865     │ ing deep neural networks with de... │ 527→602        │ dp           │ 0.71→0.96      │
│ 1889     │ faster, stronger.                   │ 480→622        │ dp           │ 1.00→1.00      │
│ 1905     │ [37] Jifeng Dai, Yi Li, Kaiming ... │ 606→624        │ dp           │ 0.80→1.00      │
│ 1927     │ 6, 2012, Lake Tahoe, Nevada, Uni... │ 603→635        │ dp           │ 0.80→1.00      │
│ 1971     │ leaved linear transformation and... │ 541→649        │ dp           │ 0.80→0.84      │
│ 1989     │ a bottleneck residual block as a... │ 600→523        │ dp           │ 0.75→1.00      │
│ 2022     │ learning frameworks. It remains ... │ 615→537        │ dp           │ 1.00→1.00      │
│ 2027     │ Using n = t we end up having to ... │ 541→535        │ dp           │ 0.80→1.00      │
│ 2123     │ information in low-dimensional s... │ 476→571        │ dp           │ 1.00→1.00      │
└──────────┴─────────────────────────────────────┴────────────────┴──────────────┴────────────────┘
... and 9 more

Regressed Alignments (23):

┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓
┃ PPX ID   ┃ PPX Text                            ┃ Mistral A→B    ┃ Method B     ┃ Conf A→B       ┃
┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩
│ 1565     │ i.                                  │ 514→516        │ dp_neighbor  │ 1.00→1.00      │
│ 1574     │ |                                   │ 548→658        │ dp_neighbor  │ 1.00→1.00      │
│ 1578     │ m−n (                               │ 548→694        │ dp           │ 1.00→0.57      │
│ 1585     │ ≥1−                                 │ 552→565        │ dp           │ 1.00→0.59      │
│ 1588     │ 2m                                  │ 552→565        │ dp_neighbor  │ 1.00→0.72      │
│ 1592     │ bility.                             │ 554→679        │ dp           │ 0.80→0.86      │
│ 1876     │ 1                                   │ 639→594        │ dp_neighbor  │ 0.82→1.00      │
│ 1885     │ 2                                   │ 531→600        │ dp_neighbor  │ 0.89→1.00      │
│ 1913     │ 7                                   │ 535→565        │ dp           │ 1.00→1.00      │
│ 1929     │ 9                                   │ 535→565        │ dp           │ 0.83→1.00      │
│ 1982     │ op∈G                                │ 599→552        │ dp_neighbor  │ 0.94→0.46      │
│ 1985     │ (2)                                 │ 590→565        │ dp           │ 1.00→1.00      │
│ 2002     │ Rs(c× s                             │ 592→527        │ dp_neighbor  │ 0.88→0.81      │
│ 2016     │ as                                  │ 594→527        │ dp_neighbor  │ 1.00→1.00      │
│ 2216     │ 2                                   │ 444→600        │ dp_neighbor  │ 0.87→1.00      │
│ 2243     │ tional neural networks. CoRR, ab... │ 446→608        │ dp           │ 1.00→0.72      │
│ 2292     │ 2017.7                              │ 468→565        │ dp           │ 0.87→0.61      │
│ 2437     │ V −                                 │ 550→673        │ dp_neighbor  │ 0.80→0.74      │
│ 2439     │ 2m                                  │ 618→654        │ dp_neighbor  │ 0.95→0.72      │
│ 2443     │ m−n                                 │ 618→590        │ dp_neighbor  │ 1.00→0.46      │
└──────────┴─────────────────────────────────────┴────────────────┴──────────────┴────────────────┘
... and 3 more
(.venv) levi@ppx:~/ppx$
```

### Analysis

Good results. Compared to run 4 (dp_context):

| Metric            | Run 4 (dp_context) | Run 6 (neighbor+monotonic) |
| ----------------- | ------------------ | -------------------------- |
| Fixed             | 28                 | 29                         |
| Regressed         | 34                 | 23                         |
| Net               | -6                 | +6                         |
| dp accuracy       | 97.5%              | 98.8%                      |
| Fallback accuracy | 20.0% (dp_context) | 63.2% (dp_neighbor)        |
| Monotonic         | -                  | 100% (3/3)                 |

The neighbor approach is a clear improvement — net positive instead of net negative, dp_neighbor at 63.2% vs dp_context's 20%, and dp_monotonic is 3/3.

Looking at the remaining 23 regressions, 12 are from dp_neighbor and 11 from base dp. The dp_neighbor regressions are almost entirely ultra-short symbolic fragments: `"i."`, `"|"`, `"2m"`, `"1"`, `"2"`, `"op∈G"`, `"as"`, `"V −"`, `"m−n"`. These are 1-2 token fragments where even the neighbor window contains multiple equally plausible Mistral candidates.

The 11 base `dp` regressions are also mostly short: `"m−n ("`, `"≥1−", "bility."`, `"7"`, `"9"`, `"(2)"`, `"2017.7"`. These are IDF re-ranking changes, not caused by the new passes.

The neighbor refinement can't reliably solve fragments this short because there are multiple Mistral fragments in the window that all contain tokens like "2" or "m". To go further, you'd likely need either:

- A minimum token length threshold to skip alignment entirely for 1-token fragments and infer them from their neighbors post-hoc
- Or stricter neighbor windowing that checks if the fragment text appears as a literal substring in the candidate Mistral text

### List runs

```bash
(.venv) levi@ppx:~/ppx$ ppx list-runs
                                                                      Alignment Runs
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓
┃ ID ┃ Version                ┃ Description                                        ┃ Mappings ┃ Accuracy ┃        Evaluated ┃ Started          ┃ Duration ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩
│  6 │ idf-neighbor-monotonic │ IDF + neighbor refinement + monotonic correction   │      939 │    97.3% │    914c/25i/939t │ 2026-02-02 12:45 │ 312.5s   │
│  5 │ baseline-no-context    │ Original IDF alignment on paper_masked (pre-run sy │      945 │    96.5% │    912c/33i/945t │ 2026-02-02 12:17 │ 0.0s     │
│  4 │ with-context           │                                                    │      942 │    95.9% │    903c/39i/942t │ 2026-01-31 05:18 │ 351.7s   │
│  3 │ with-context           │ IDF + spatial context fallback                     │     1505 │    87.2% │ 1312c/193i/1505t │ 2026-01-31 03:13 │ 707.6s   │
└────┴────────────────────────┴────────────────────────────────────────────────────┴──────────┴──────────┴──────────────────┴──────────────────┴──────────┘
(.venv) levi@ppx:~/ppx$
```
