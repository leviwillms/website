# New Year, Same Problem

## Introduction

It is a new year and that means it's time to get back into some research on ppx. Picking up where I left off should be straightforward since I mapped out some potential algorithms in my last research session. My goal is to put together an algorithm that improves the analysis from the 68% matching I currently have.

# Next Steps

I have run my proposed solution through Claude Opus 4.5 and received some quality feedback. The main point is to not use a greedy algorithm but instead use Dynamic Programming to receive a globally optimized solution. I agree that this is a solid approach.

Claude also suggested a modified LCS which I find interesting, as that is something I was originally going to attempt.

I will now look to implement tokenization in the ppx codebase and write out this algorithm. Claude's response attached below for reference.

```
Review of Your Token-Based Alignment Algorithm
Current State Analysis
Your Problem: You're trying to align spatially-positioned text fragments (PPX with bounding boxes) to a continuous markdown text stream (Mistral OCR). Current ILIKE matching achieves 68.11% match rate, leaving 32% unmatched. Your Proposed Solution: Use tokenization to find optimal sub-sequences in markdown that match text fragments, even when the fragments are split across lines or have minor variations.
Feedback on tokenizer/main.py
Critical Issues
Token Counting is Flawed (lines 41-44)

for id in encoded_text2.input_ids:
    if id in encoded_text1.input_ids:
        num_equivalences += 1
This counts token set intersection, NOT sequential alignment. If text1 = "cat dog" and text2 = "dog cat", this would report high equivalence despite reversed order.
Missing Position Information Your pseudocode (lines 107-128) mentions tracking left, right, start, end but the current implementation discards positional information entirely.
Algorithmic Confusion The pseudocode suggests you want to find the shortest span in the Mistral fragment that contains the most tokens from the PPX fragment. This is actually a sliding window problem, not just token counting.
Algorithm Recommendations
Given your problem structure, I recommend a Dynamic Programming approach over greedy. Here's why:
Why DP Over Greedy?
Greedy Limitations:
May miss optimal alignment when tokens appear multiple times
Can't handle gaps (missing/extra tokens) optimally
Local decisions don't guarantee global optimality
DP Advantages:
Handles partial matches and gaps systematically
Finds globally optimal alignment
Can track edit distance + position simultaneously
Better for handling OCR variations (substitutions, deletions)
Recommended Algorithm: Modified Longest Common Subsequence (LCS) with Position Tracking
This is essentially a token-level sequence alignment problem, similar to bioinformatics sequence alignment.
Algorithm Structure

def find_best_alignment(ppx_tokens, mistral_tokens):
    """
    Find the best alignment using DP with position tracking.

    Returns:
        - match_score: number of matching tokens
        - start_idx: start position in mistral_tokens
        - end_idx: end position in mistral_tokens
        - alignment: list of (ppx_idx, mistral_idx) pairs
    """
    n, m = len(ppx_tokens), len(mistral_tokens)

    # dp[i][j] = (score, start_pos) for best alignment ending at mistral[j]
    # using first i tokens from ppx
    dp = [[{0, -1} for _ in range(m + 1)] for _ in range(n + 1)]

    best_score = 0
    best_end = -1
    best_start = -1

    for i in range(1, n + 1):
        for j in range(1, m + 1):
            # Match case
            if ppx_tokens[i-1] == mistral_tokens[j-1]:
                if dp[i-1][j-1][0] == 0:
                    # Starting new match
                    dp[i][j] = (1, j-1)
                else:
                    # Continuing match
                    dp[i][j] = (dp[i-1][j-1][0] + 1, dp[i-1][j-1][1])
            else:
                # Gap in mistral (skip mistral token)
                dp[i][j] = max(
                    (dp[i][j-1][0], dp[i][j-1][1]),
                    (dp[i-1][j][0], dp[i-1][j][1])
                )

            # Track best alignment with minimum span
            score, start = dp[i][j]
            span = j - start

            if i == n:  # Only consider complete PPX fragment
                if (score > best_score or
                    (score == best_score and span < best_end - best_start)):
                    best_score = score
                    best_start = start
                    best_end = j

    return best_score, best_start, best_end
Key Features
Position Tracking: Tracks where match sequence starts in mistral text
Gap Handling: Allows missing tokens in either sequence
Optimal Substructure: DP ensures global optimality
Span Minimization: Prefers shorter spans for same match count
Implementation Roadmap
Phase 1: Token-Based Alignment (DP)

def align_fragment_to_mistral(ppx_fragment: TextFragment,
                              mistral_fragments: List[MistralTextFragment],
                              tokenizer) -> AlignmentResult:
    """
    Find best alignment using DP across all mistral fragments.
    """
    ppx_tokens = tokenizer(ppx_fragment.text).input_ids

    best_alignment = None
    best_score = 0

    for mf in mistral_fragments:
        mf_tokens = tokenizer(mf.text).input_ids
        score, start, end = find_best_alignment(ppx_tokens, mf_tokens)

        # Normalize by fragment length
        normalized_score = score / len(ppx_tokens)

        if normalized_score > best_score:
            best_score = normalized_score
            best_alignment = AlignmentResult(
                mistral_fragment_id=mf.id,
                token_start=start,
                token_end=end,
                match_score=score,
                confidence=normalized_score
            )

    return best_alignment
Phase 2: Preprocessing Optimizations
Token Index Building (critical for performance):

def build_token_index(mistral_fragments, tokenizer):
    """
    Build inverted index: token_id -> [(fragment_id, position), ...]
    """
    token_index = defaultdict(list)

    for mf in mistral_fragments:
        tokens = tokenizer(mf.text).input_ids
        for pos, token_id in enumerate(tokens):
            token_index[token_id].append((mf.id, pos))

    return token_index
This allows you to:
Quickly filter candidate fragments (those sharing tokens)
Skip fragments with too few shared tokens
Reduce from O(n²) to O(k) where k = candidates with shared tokens
Phase 3: Handling Word Boundaries
Your current issue with trailing hyphens needs better text normalization:

def normalize_text(text: str) -> str:
    """
    Handle hyphenation, whitespace, and common OCR issues.
    """
    # Remove trailing hyphens at line breaks
    text = re.sub(r'-\s+', '', text)

    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text)

    # Handle common OCR substitutions
    substitutions = {
        'l': '1',  # lowercase L vs one
        'O': '0',  # capital O vs zero
        # Add more as you discover patterns
    }

    return text.strip()
Performance Considerations
Current Bottlenecks in Your Analyzer Code
Database Query in Loop (tokenizer/main.py:107-128)

for tf in tfs:
    stmt = select(...).where(MistralTextFragment.text.ilike(f'%{s}%'))
    result = session.execute(stmt).first()
This is O(n) queries. Batch load all mistral fragments once.
find_closest_match Loading All Fragments

results = session.execute(stmt).scalars().all()
This loads the ENTIRE mistral corpus for every unmatched fragment. Use the token index instead.
Optimized Flow

def run_optimized_alignment(session, filename, page_range):
    # 1. Load ALL data once
    pages = get_pages(session, filename, start, end).all()
    all_ppx_fragments = []
    all_mistral_fragments = []

    for page in pages:
        all_ppx_fragments.extend(get_text_fragments(session, filename, page.id))
        all_mistral_fragments.extend(
            session.query(MistralTextFragment)
                   .filter_by(page_id=page.id)
                   .all()
        )

    # 2. Build token index
    token_index = build_token_index(all_mistral_fragments, tokenizer)

    # 3. Process each PPX fragment
    for ppx_frag in all_ppx_fragments:
        # Use token index to get candidate mistral fragments
        candidates = get_candidate_fragments(ppx_frag, token_index, all_mistral_fragments)

        # Run DP alignment only on candidates
        best_match = align_fragment_to_mistral(ppx_frag, candidates, tokenizer)

        # Store alignment result
Performance Improvement: O(n²) → O(n·k) where k << n
Additional Recommendations
Evaluation Metrics: Beyond just match rate, track:
Position accuracy (if you have ground truth)
Span efficiency (matched tokens / span length)
Distribution of confidence scores
Database Schema Addition:

class FragmentAlignment(Base):
    __tablename__ = 'fragment_alignments'

    id = Column(Integer, primary_key=True)
    ppx_fragment_id = Column(ForeignKey('text_fragments.id'))
    mistral_fragment_id = Column(ForeignKey('mistral_text_fragments.id'))
    token_start = Column(Integer)
    token_end = Column(Integer)
    match_score = Column(Float)
    confidence = Column(Float)
    alignment_method = Column(String)  # 'exact', 'dp', 'fuzzy'
Fallback Strategy:
Threshold 1 (>0.9): High confidence, use alignment
Threshold 2 (0.7-0.9): Medium confidence, flag for review
Below 0.7: Use multimodal/BERT approach you mentioned
Conclusion
Your intuition is correct: Token-based alignment can solve this problem without heavy ML models. However:
Use DP, not greedy - The global optimization is necessary
Build a token index - Essential for performance at scale
Fix normalization - Handle hyphens, whitespace systematically
Batch operations - Eliminate per-fragment database queries
The DP approach will likely push your match rate from 68% to 85-90%, with the remaining cases being true OCR errors or structural mismatches that may need multimodal approaches.

```
