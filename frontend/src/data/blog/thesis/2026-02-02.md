# Post-Alignment Steps

## Update

With a satisfactory level of alignment now achieved I will begin working on the query/embedding piece. With consideration of using this system for 1000+ page textbooks, we must think through the following:

1. (4) Hybrid BM25 + Dense — moves from "recommended" to "necessary." At textbook scale the dense retriever alone won't reliably surface the right fragments when dozens score within 0.02 of each other. BM25 breaks ties on exact terminology, which matters heavily for textbooks (definitions, theorems, named methods).

2. (5) Re-ranking — also becomes essential rather than optional. With 60k fragments, your initial retrieval (dense + BM25) should pull back ~100-200 candidates, then a cross-encoder re-ranker sorts them properly. Without this, results from Chapter 2 and Chapter 15 that both mention a concept will be interleaved without useful ordering.

3. (6) Knowledge graph — actually becomes more interesting at textbook scale. Textbooks have explicit structure (chapters, sections, subsections, index terms, cross-references) that academic papers don't. You wouldn't need a general-purpose KG — a lightweight document structure graph (section hierarchy + entity mentions) could enable scoped queries like "gradient descent in the optimization chapter" vs. "gradient descent in the neural networks chapter." Still the highest implementation cost of the group.

First, prior to these implementation I want to optimize alignment and verification (through OpenAI) to run parallel at scale.
